[2024-11-18T07:14:34.612+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-11-18T07:14:33.799751+00:00 [queued]>
[2024-11-18T07:14:34.638+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-11-18T07:14:33.799751+00:00 [queued]>
[2024-11-18T07:14:34.638+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-11-18T07:14:34.654+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): reddit_extraction> on 2024-11-18 07:14:33.799751+00:00
[2024-11-18T07:14:34.658+0000] {standard_task_runner.py:57} INFO - Started process 81 to run task
[2024-11-18T07:14:34.662+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_extraction', 'manual__2024-11-18T07:14:33.799751+00:00', '--job-id', '14', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmpg8u_sgz4']
[2024-11-18T07:14:34.663+0000] {standard_task_runner.py:85} INFO - Job 14: Subtask reddit_extraction
[2024-11-18T07:14:34.710+0000] {task_command.py:416} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-11-18T07:14:33.799751+00:00 [running]> on host 9f42dd3b4e06
[2024-11-18T07:14:34.885+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Bao Nguyen' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-11-18T07:14:33.799751+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-11-18T07:14:33.799751+00:00'
[2024-11-18T07:14:34.888+0000] {logging_mixin.py:151} WARNING - Version 7.7.1 of praw is outdated. Version 7.8.1 was released Friday October 25, 2024.
[2024-11-18T07:14:34.890+0000] {logging_mixin.py:151} INFO - connected to reddit!
[2024-11-18T07:14:35.623+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f8eedd99ac0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'So my current thought is do a bit of both. If something has a fivetran connector, load it straight into Snowflake, but if doesn’t, load it to S3 then snow pipe it into snowflake.\n\nAny thoughts on this? Has anyone found loading straight into snowflake something they regret doing with an ingestion tool like fivetran? I’m thinking if you need to re-ingest data, or whatever else?\n\nWhen I’ve worked with warehouses like Redshift, I’ve ALWAYS loaded to S3 first, but it seems like loading straight into snowflake is the way a lot of people go.', 'author_fullname': 't2_lodwnfe3v', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Do you ingest to S3 or straight to snowflake?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gt92wf', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.94, 'author_flair_background_color': 'transparent', 'subreddit_type': 'public', 'ups': 49, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': '980947cc-e787-11ed-87e3-ae81ee052dfe', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 49, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731831639.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>So my current thought is do a bit of both. If something has a fivetran connector, load it straight into Snowflake, but if doesn’t, load it to S3 then snow pipe it into snowflake.</p>\n\n<p>Any thoughts on this? Has anyone found loading straight into snowflake something they regret doing with an ingestion tool like fivetran? I’m thinking if you need to re-ingest data, or whatever else?</p>\n\n<p>When I’ve worked with warehouses like Redshift, I’ve ALWAYS loaded to S3 first, but it seems like loading straight into snowflake is the way a lot of people go.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Principal Data Engineer', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1gt92wf', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='General-Parsnip3138'), 'discussion_type': None, 'num_comments': 38, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1gt92wf/do_you_ingest_to_s3_or_straight_to_snowflake/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gt92wf/do_you_ingest_to_s3_or_straight_to_snowflake/', 'subreddit_subscribers': 229845, 'created_utc': 1731831639.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-18T07:14:35.624+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 8, in reddit_pipeline
    posts = extract_posts(instance, subreddit, time_filter, limit)
  File "/opt/airflow/etls/reddit_etl.py", line 27, in extract_posts
    post = {key: post_dict[key] for key in POST_FIELDS}
NameError: name 'POST_FIELDS' is not defined
[2024-11-18T07:14:35.633+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=etl_reddit_pipeline, task_id=reddit_extraction, execution_date=20241118T071433, start_date=20241118T071434, end_date=20241118T071435
[2024-11-18T07:14:35.643+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 14 for task reddit_extraction (name 'POST_FIELDS' is not defined; 81)
[2024-11-18T07:14:35.670+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-11-18T07:14:35.686+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
